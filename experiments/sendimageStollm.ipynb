{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de977d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5967e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model= \"gemini-2.0-flash\",\n",
    "    convert_system_message_to_human=True,\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0064d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_llm(image_file_path):\n",
    "    with open(image_file_path, \"rb\") as image_file:\n",
    "        image_data = image_file.read()\n",
    "    encoded_image = base64.b64encode(image_data).decode('utf-8')\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": \"Describe the image I am sending. Send it as plain text in paragraphs with no formatting\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/png;base64,{encoded_image}\"}\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = llm.invoke([message])\n",
    "    parsed_response = output_parser.invoke(response)\n",
    "    return (response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f517aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = './output_images'\n",
    "for i in range(5):\n",
    "    llm_output = \"\"\n",
    "    image_filename = os.path.join(output_path,f'page_{i+1:03}.png')\n",
    "    llm_output = img_to_llm(image_filename)\n",
    "    text_filename = os.path.join(output_path, f'page_{i+1:03}.txt')\n",
    "    with open(text_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(llm_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63e462eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "# Define the model and its settings\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {'device': 'cuda'}  # If you want to use GPU (CUDA)\n",
    "encode_kwargs = {'normalize_embeddings': True}  # For cosine similarity\n",
    "\n",
    "# Create the embeddings object\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,  # You can change this for different tasks\n",
    "    query_instruction = \"Represent this sentence for answering questions:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d7dadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter()\n",
    "file_names = sorted([f for f in os.listdir(output_path) if f.endswith('.txt')])\n",
    "documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "961e43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_names:\n",
    "    with open(os.path.join(output_path, file_name), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    chunks = splitter.split_text(text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\"source\": file_name, \"chunk_id\": i}\n",
    "        ))\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a281cfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Use the following context to answer the question at the end. \n",
    "Answer in paragraphs with no formatting. Headings can be shown with the help of colons.\n",
    "If the answer is not in the context, just say \"I don't know\"â€” do not make anything up.\n",
    "If the answer is in the context and the user specifies the statement \"AOFS\" which stands for \"Answer only from slides\" do not provide any other information other than what is in the context.\n",
    "In the event user does not specify that statement and If the answer is in the context, Explain the context using your own knowledge of that subject, it doesn't have to be in the context.\n",
    "Also include a fun explanation to the concept using different analogies.\n",
    "Finally, include a very brief intuitive explanation which helps me get the basic intution of what's going on.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\n",
    "Answer:\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ec2bbd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4993ccae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Synchronous data transfer is the transfer of data between two devices on a network where they both carry out a \n",
       "predetermined set of interactions based on a common clock pulse. Asynchronous data transfer is the transfer of data\n",
       "between two devices on a network where they both carry out a predetermined set of interactions based on a private \n",
       "clock pulse.\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Synchronous data transfer is the transfer of data between two devices on a network where they both carry out a \n",
       "predetermined set of interactions based on a common clock pulse. Asynchronous data transfer is the transfer of data\n",
       "between two devices on a network where they both carry out a predetermined set of interactions based on a private \n",
       "clock pulse.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\":\"Describe the difference between Synchronous and Asyncrhonous data transfer AOFS.\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
